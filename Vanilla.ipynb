{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras import callbacks\n",
    "from keras.datasets import mnist\n",
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "from keras.optimizers import *\n",
    "from keras import backend as K\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 190, 1536)\n",
      "(1000,)\n"
     ]
    }
   ],
   "source": [
    "RN_input_small = np.load('RN_input_small.npy')\n",
    "a_small = np.load('a_small.npy')\n",
    "print (RN_input_small.shape)\n",
    "print (a_small.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(a_small)\n",
    "train_labels = le.transform(a_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(train_labels)\n",
    "l = len(np.unique(train_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_sent_max_len = 12\n",
    "q_sent_max_len = 12\n",
    "embedding_size = 32\n",
    "LSTM_size = 256\n",
    "batch_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_RN_input(embedded_c, embedded_q, answer):\n",
    "    \"\"\"\n",
    "    Returns the input for RN which has dim (n, 190, 1536), \n",
    "    where n is the number of context and 190=20C2.\n",
    "    Input(s):\n",
    "    - embedded_c: list of context (up to 20 sentences) \n",
    "                  for each different question.  \n",
    "    - embedded_q: embedding for the related question to \n",
    "                  the given embedded_c.\n",
    "    - answer: corresponding answer to the embedded c and q.              \n",
    "    \"\"\"\n",
    "    object_pairs = []\n",
    "\n",
    "    for i in embedded_c:\n",
    "        object_pair = list(itertools.combinations(i, 2))\n",
    "        object_pairs.append(object_pair)\n",
    "    \n",
    "    rn_inputs = []\n",
    "    m = len(object_pairs)\n",
    "    z = np.zeros((m, 190, 1536))    \n",
    "    \n",
    "    for i, object_pair in enumerate(object_pairs):\n",
    "        n = len(object_pair)\n",
    "        for j in range(n):\n",
    "            l = np.array(object_pair[j][0] + object_pair[j][1] + embedded_q[i][0])\n",
    "#             z[i][j] = np.array(object_pair[j][0] + object_pair[j][1] + embedded_q[i][0])\n",
    "            z[i][j] = np.concatenate((object_pair[j][0], object_pair[j][1], embedded_q[i][0]))\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(None), Dimension(512)])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RN_inputs.get_shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 1 required positional argument: 'target_shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-9d66331b4400>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# RN_inputs = Lambda(convert_to_RN_input, arguments={c_lstm,q_lstm})\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mRN_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc_lstm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq_lstm\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mRN_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mReshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mfactor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() missing 1 required positional argument: 'target_shape'"
     ]
    }
   ],
   "source": [
    "c_input = Input(shape=(c_sent_max_len,))\n",
    "c_embed = Embedding(c_sent_max_len, embedding_size)(c_input)\n",
    "c_lstm = LSTM(LSTM_size)(c_embed)\n",
    "\n",
    "q_input = Input(shape=(q_sent_max_len,))\n",
    "q_embed = Embedding(q_sent_max_len, embedding_size)(q_input)\n",
    "q_lstm = LSTM(LSTM_size)(q_embed)\n",
    "\n",
    "# RN_inputs = Lambda(convert_to_RN_input, arguments={c_lstm,q_lstm})\n",
    "RN_inputs = K.concatenate([c_lstm, q_lstm])\n",
    "RN_inputs = Reshape()\n",
    "\n",
    "factor = 1\n",
    "\n",
    "g_units = [256,256,256,256]*factor\n",
    "\n",
    "g1 = Dense(g_units[0], activation='relu')(RN_inputs)\n",
    "g2 = Dense(g_units[1], activation='relu')(g1)\n",
    "g3 = Dense(g_units[2], activation='relu')(g2)\n",
    "g4 = Dense(g_units[3], activation='relu')(g3)\n",
    "g_sum = Lambda(lambda x : K.sum(x,axis=1))(g4)\n",
    "\n",
    "f_units = [256*factor, 512*factor, l]\n",
    "\n",
    "f1 = Dense(f_units[0], activation='relu')(g_sum)\n",
    "f2 = Dense(f_units[1], activation='relu')(f1)\n",
    "f3 = Dense(f_units[2], activation='softmax')(f2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(inputs=[c_input, q_input], outputs = f3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
